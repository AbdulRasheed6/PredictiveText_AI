# PredictiveText_AI

Text predicition have become popular due to the capabilities of new ML model like Transformers, Bidirectional Encoder Representations.Although simple RNNS (Recurrent Neural Networks) were used for training text data. But in recent years there have been many new research publications that provide state-of-the-art results. One of such is BERT.
BERT stands for Bidirectional Encoder Representations from Transformers, but in order to have a deeper sense of language context, BERT uses bidirectional training. Sometimes, it’s also referred to as “non-directional”. So,  it takes both the previous and next tokens into account simultaneously. BERT applies the bidirectional training of Transformer to language modeling, learns the text representations. It is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context.

![bert](https://user-images.githubusercontent.com/59423092/230950403-a4238967-ca3c-4ae7-adf7-9e8ec422b7da.png)








